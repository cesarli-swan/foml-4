{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________\n",
    "---------------\n",
    "| ENVIRONMENT |\n",
    "---------------\n",
    "_________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_args = {\n",
    "    \"id\" : \"FrozenLake-v1\",\n",
    "    \"map_name\" : \"4x4\",\n",
    "    \"is_slippery\" : False,\n",
    "    \"render_mode\" : \"human\"\n",
    "}\n",
    "env = Monitor(gym.make(**env_args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________\n",
    "-------------------\n",
    "| HYPERPARAMETERS |\n",
    "-------------------\n",
    "\n",
    "The Deep Q-Network hyperparameters determine how the agent behaves and how the \n",
    "policy is updated.\n",
    "\n",
    "Many of these values come from Q-tables. Many others were introduced for the \n",
    "neural network approach.\n",
    "\n",
    "For more information, visit:\n",
    "\n",
    "https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#\n",
    "_________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dqn_args =  {\n",
    "    \"policy\" : \"MlpPolicy\", \n",
    "    \"learning_rate\" : 0.0007,\n",
    "    \"buffer_size\" : 10_000,\n",
    "    \"learning_starts\" : 100,\n",
    "    \"target_update_interval\" : 1_000,\n",
    "    \"gamma\" : 0.99,\n",
    "    \"train_freq\" : 4,\n",
    "    \"tau\" : 1.0,\n",
    "    \"gradient_steps\" : 1,\n",
    "    \"exploration_fraction\" : 0.1,\n",
    "    \"exploration_initial_eps\" : 1.0,\n",
    "    \"exploration_final_eps\" : 0.05,\n",
    "    \"batch_size\" : 32,\n",
    "    \"verbose\" : 1\n",
    "\n",
    "}\n",
    "model = DQN(env= env, **dqn_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________\n",
    "------------\n",
    "| TRAINING |\n",
    "------------\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "`eval_freq` : int\n",
    "    Number of training timesteps before evaluating the agent's performance\n",
    "    on a fresh environment without updating the policy.\n",
    "\n",
    "`deterministic` : bool\n",
    "    Indicates whether the agent should ever act randomly or not during evaluation.\n",
    "\n",
    "`n_eval_episodes` : int\n",
    "    Number of episodes to perform evaluation before calculating the mean reward.\n",
    "\n",
    "`total_timesteps` : int\n",
    "    Determines the number of steps the agent will take before training ends.\n",
    "\n",
    "`callback` : EvalCallback\n",
    "    A callback to intermittently perform agent evaluation\n",
    "\n",
    "`log_interval` : int \n",
    "    Determines the number of timesteps before printing training stats to \n",
    "    stdout.\n",
    "_________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 19:13:31.342 Python[81639:31890251] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2024-12-11 19:13:31.342 Python[81639:31890251] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.83     |\n",
      "|    ep_rew_mean      | 0.59     |\n",
      "|    exploration_rate | 0.147    |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 3        |\n",
      "|    time_elapsed     | 1255     |\n",
      "|    total_timesteps  | 4491     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 8.22e-06 |\n",
      "|    n_updates        | 1097     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.2     |\n",
      "|    ep_rew_mean      | 0.79     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1000     |\n",
      "|    fps              | 3        |\n",
      "|    time_elapsed     | 2548     |\n",
      "|    total_timesteps  | 9128     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 3.79e-07 |\n",
      "|    n_updates        | 2256     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 6.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6        |\n",
      "|    mean_reward      | 1        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 7.79e-09 |\n",
      "|    n_updates        | 2474     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.14     |\n",
      "|    ep_rew_mean      | 0.95     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1500     |\n",
      "|    fps              | 3        |\n",
      "|    time_elapsed     | 3492     |\n",
      "|    total_timesteps  | 12210    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 3.91e-08 |\n",
      "|    n_updates        | 3027     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.09     |\n",
      "|    ep_rew_mean      | 0.97     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2000     |\n",
      "|    fps              | 3        |\n",
      "|    time_elapsed     | 4384     |\n",
      "|    total_timesteps  | 15265    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 1.94e-11 |\n",
      "|    n_updates        | 3791     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.06     |\n",
      "|    ep_rew_mean      | 0.92     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2500     |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 26875    |\n",
      "|    total_timesteps  | 18321    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 5.47e-14 |\n",
      "|    n_updates        | 4555     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 6.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6        |\n",
      "|    mean_reward      | 1        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 3.01e-15 |\n",
      "|    n_updates        | 4974     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.15     |\n",
      "|    ep_rew_mean      | 0.95     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3000     |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 53705    |\n",
      "|    total_timesteps  | 21407    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 1.78e-15 |\n",
      "|    n_updates        | 5326     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.45     |\n",
      "|    ep_rew_mean      | 0.96     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3500     |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 55501    |\n",
      "|    total_timesteps  | 24473    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 1.7e-07  |\n",
      "|    n_updates        | 6093     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.54     |\n",
      "|    ep_rew_mean      | 0.97     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4000     |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 56479    |\n",
      "|    total_timesteps  | 27864    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 2.93e-07 |\n",
      "|    n_updates        | 6940     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 100      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 3.7e-06  |\n",
      "|    n_updates        | 7474     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.78     |\n",
      "|    ep_rew_mean      | 0.9      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4500     |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 58571    |\n",
      "|    total_timesteps  | 33149    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 7.27e-06 |\n",
      "|    n_updates        | 8262     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.11     |\n",
      "|    ep_rew_mean      | 0.95     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5000     |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 59810    |\n",
      "|    total_timesteps  | 37572    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 1.53e-06 |\n",
      "|    n_updates        | 9367     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 100      |\n",
      "|    mean_reward      | 0        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 1.23e-05 |\n",
      "|    n_updates        | 9974     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.13     |\n",
      "|    ep_rew_mean      | 0.93     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 5500     |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 61486    |\n",
      "|    total_timesteps  | 41212    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 2.99e-06 |\n",
      "|    n_updates        | 10277    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.3      |\n",
      "|    ep_rew_mean      | 0.95     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6000     |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 62646    |\n",
      "|    total_timesteps  | 45317    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 6.74e-07 |\n",
      "|    n_updates        | 11304    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.14     |\n",
      "|    ep_rew_mean      | 0.92     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 6500     |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 63830    |\n",
      "|    total_timesteps  | 49522    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 1e-06    |\n",
      "|    n_updates        | 12355    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 6.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 6        |\n",
      "|    mean_reward      | 1        |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 50000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0007   |\n",
      "|    loss             | 7.83e-07 |\n",
      "|    n_updates        | 12474    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x16b4ea7e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_args = {\n",
    "    \"eval_freq\" : 10_000, \n",
    "    \"deterministic\" : True,\n",
    "    \"n_eval_episodes\" : 25\n",
    "}\n",
    "eval_callback = EvalCallback(env, **callback_args)\n",
    "\n",
    "train_args = {\n",
    "    \"total_timesteps\" : 1_000,\n",
    "    \"callback\" : eval_callback,\n",
    "    \"log_interval\" : 100\n",
    "}\n",
    "model.learn(**train_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________\n",
    "--------------\n",
    "| EVALUATION |\n",
    "--------------\n",
    "\n",
    "Performs the final model evaluation after training is complete.\n",
    "_________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation:\n",
      "> Mean Reward: 1.0\n",
      "> Reward STD: 0.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "n_eval_eps = 100\n",
    "mean_rwd, rwd_std = evaluate_policy(model= model,\n",
    "                                    env= env,\n",
    "                                    deterministic= True,\n",
    "                                    n_eval_episodes= n_eval_eps,\n",
    "                                    render= True)\n",
    "\n",
    "print(f\"Final Evaluation:\\n> Mean Reward: {mean_rwd}\\n> Reward STD: {rwd_std}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
